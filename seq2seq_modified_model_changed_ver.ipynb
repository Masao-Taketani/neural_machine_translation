{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "#tmp order\n",
    "from keras.layers import Input, CuDNNLSTM, Embedding, Dense, Dropout, RNN, LSTMCell, TimeDistributed\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "###\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "hidden_dims = 256\n",
    "\n",
    "data_file = \"jpn.txt\"\n",
    "enc_input_tokens = []\n",
    "dec_input_tokens = []\n",
    "dec_target_tokens = []\n",
    "start_token_id = 1\n",
    "end_token_id = 2\n",
    "pad_token_id = 3\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_list = f.read().split(\"\\n\")\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"sentencepiece/spm_for_nmt.model\")\n",
    "\n",
    "for line in lines_list:\n",
    "    #for the last black data, we need to skip\n",
    "    if line == \"\":\n",
    "        break\n",
    "    source_text, target_text = line.split(\"\\t\")\n",
    "    tokenized_source_text = tokenizer.EncodeAsPieces(source_text)\n",
    "    #test = tokenizer.EncodeAsIds(source_text)\n",
    "    #print(test)\n",
    "    tokenized_target_text = tokenizer.EncodeAsPieces(target_text)\n",
    "    \n",
    "    int_tokenized_source = []\n",
    "    int_tokenized_input_target = []\n",
    "    int_tokenized_output_target = []\n",
    "    for token in tokenized_source_text:\n",
    "        int_tokenized_source.append(tokenizer.piece_to_id(token))\n",
    "    for i, token in enumerate(tokenized_target_text):\n",
    "        if i == 0:\n",
    "            int_tokenized_input_target.append(start_token_id)\n",
    "            continue\n",
    "        int_tokenized_input_target.append(tokenizer.piece_to_id(token))\n",
    "        int_tokenized_output_target.append(tokenizer.piece_to_id(token))\n",
    "        \n",
    "    int_tokenized_output_target.append(end_token_id)\n",
    "    \n",
    "    if len(int_tokenized_output_target) != len(int_tokenized_input_target):\n",
    "        print(\"Error\")\n",
    "        \n",
    "    enc_input_tokens.append(int_tokenized_source)\n",
    "    dec_input_tokens.append(int_tokenized_input_target)\n",
    "    dec_target_tokens.append(int_tokenized_output_target)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_enc_seq = max([len(i) for i in enc_input_tokens])\n",
    "max_dec_seq = max([len(i) for i in dec_input_tokens])\n",
    "\n",
    "max_enc_seq, max_dec_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(pad_token_id)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45093/45093 [00:00<00:00, 45910.31it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 84701.79it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 85467.92it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_tokens = pad_or_truncate_inputs(enc_input_tokens, max_enc_seq)\n",
    "dec_input_tokens = pad_or_truncate_inputs(dec_input_tokens, max_dec_seq)\n",
    "dec_target_tokens = pad_or_truncate_inputs(dec_target_tokens, max_dec_seq)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 168, 421, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] \n",
      " [168, 421, 6, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(dec_input_tokens[0],\"\\n\", dec_target_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def shuffle_dataset_and_split_into_train_test(enc_input, dec_input, dec_target, test_ratio=0.2):\n",
    "    dataset_list = list(zip(enc_input, dec_input, dec_target))\n",
    "    np.random.shuffle(dataset_list)\n",
    "    split_point = int(len(enc_input) * test_ratio)\n",
    "    test = dataset_list[:split_point]\n",
    "    train = dataset_list[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36075, 9018, 3, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = shuffle_dataset_and_split_into_train_test(enc_input_tokens, dec_input_tokens, dec_target_tokens)\n",
    "len(train), len(test), len(train[0]), len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61, 61)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0][0]), len(train[0][1]), len(train[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_list, batch_size, shuffle=False):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data_list)\n",
    "            \n",
    "        for i in range(0, len(data_list), batch_size):\n",
    "            enc_input_list = []\n",
    "            dec_input_list = []\n",
    "            dec_target_list = []\n",
    "            batch_list_inside_tuples = data_list[i: i + batch_size]\n",
    "            \n",
    "            for sample in batch_list_inside_tuples:\n",
    "                e_inp, d_inp, d_tar = sample[0], sample[1], sample[2]\n",
    "                enc_input_list.append(e_inp)\n",
    "                dec_input_list.append(d_inp)\n",
    "                dec_target_list.append(d_tar)\n",
    "            np_batch_enc_input = np.vstack(enc_input_list)\n",
    "            np_batch_dec_input = np.vstack(dec_input_list)\n",
    "            np_batch_dec_target = np.vstack(dec_target_list)\n",
    "            np_batch_dec_target = np_batch_dec_target.reshape((np_batch_dec_target.shape[0], np_batch_dec_target.shape[1], 1))\n",
    "            #np_batch_dec_target_one_hot = to_categorical(np_batch_dec_target, num_classes=vocab_in_size)\n",
    "            ##input values are inside of [], and the rest is output value\n",
    "            #yield [np_batch_enc_input, np_batch_dec_input], np_batch_dec_target_one_hot\n",
    "            yield [np_batch_enc_input, np_batch_dec_input], np_batch_dec_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 140)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_batch = generate_data(train, batch_size, shuffle=True)\n",
    "test_on_batch = generate_data(test, batch_size)\n",
    "train_steps_per_epoch = len(train) // batch_size\n",
    "test_steps_per_epoch = len(test) // batch_size\n",
    "train_steps_per_epoch, test_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = 8000\n",
    "vocab_out_size = 8000\n",
    "units = 256\n",
    "embedding_dim = 300\n",
    "len_input = max_enc_seq\n",
    "\n",
    "encoder_inputs = Input(shape=(len_input,))\n",
    "encoder_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "encoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_emb(encoder_inputs))\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Now create the Decoder layers.\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "decoder_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "decoder_lstm_out, _, _ = decoder_lstm(decoder_emb(decoder_inputs), initial_state=encoder_states)\n",
    "# Two dense layers added to this model to improve inference capabilities.\n",
    "decoder_d1 = Dense(units, activation=\"relu\")\n",
    "decoder_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "# Drop-out is added in the dense layers to help mitigate overfitting in this part of the model. Astute developers\n",
    "# may want to add the same mechanism inside the LSTMs.\n",
    "decoder_out = decoder_d2(Dropout(rate=.4)(decoder_d1(Dropout(rate=.4)(decoder_lstm_out))))\n",
    "\n",
    "# Finally, create a training model which combines the encoder and the decoder.\n",
    "# Note that this model has three inputs:\n",
    "#  encoder_inputs=[batch,encoded_words] from input language (English)\n",
    "#  decoder_inputs=[batch,encoded_words] from output language (Spanish). This is the \"teacher tensor\".\n",
    "#  decoder_out=[batch,encoded_words] from output language (Spanish). This is the \"target tensor\".\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_out)\n",
    "plot_model(model, to_file=\"check.png\", show_shapes=True)\n",
    "# We'll use sparse_categorical_crossentropy so we don't have to expand decoder_out into a massive one-hot array.\n",
    "#  Adam is used because it's, well, the best.\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", #metrics=['sparse_categorical_accuracy'])\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list, a = next(train_on_batch)\n",
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 27s 49ms/step - loss: 1.3398 - val_loss: 0.9740\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.9319 - val_loss: 0.8810\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.8620 - val_loss: 0.8212\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.8094 - val_loss: 0.7751\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 25s 45ms/step - loss: 0.7669 - val_loss: 0.7389\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.7316 - val_loss: 0.7121\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.7047 - val_loss: 0.6911\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6811 - val_loss: 0.6752\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6640 - val_loss: 0.6636\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6456 - val_loss: 0.6504\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6306 - val_loss: 0.6405\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6190 - val_loss: 0.6349\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.6074 - val_loss: 0.6285\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.5965 - val_loss: 0.6250\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - 24s 43ms/step - loss: 0.5866 - val_loss: 0.6164\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5759 - val_loss: 0.6123\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5680 - val_loss: 0.6140\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5605 - val_loss: 0.6028\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5483 - val_loss: 0.5990\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5405 - val_loss: 0.5957\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5330 - val_loss: 0.5924\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5257 - val_loss: 0.5900\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5204 - val_loss: 0.5871\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5148 - val_loss: 0.5865\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5089 - val_loss: 0.5868\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.5041 - val_loss: 0.5843\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4993 - val_loss: 0.5832\n",
      "Epoch 28/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4943 - val_loss: 0.5831\n",
      "Epoch 29/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4905 - val_loss: 0.5835\n",
      "Epoch 30/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4860 - val_loss: 0.5815\n",
      "Epoch 31/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4812 - val_loss: 0.5820\n",
      "Epoch 32/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4777 - val_loss: 0.5801\n",
      "Epoch 33/100\n",
      "563/563 [==============================] - 25s 45ms/step - loss: 0.4737 - val_loss: 0.5803\n",
      "Epoch 34/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4703 - val_loss: 0.5788\n",
      "Epoch 35/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4655 - val_loss: 0.5810\n",
      "Epoch 36/100\n",
      "563/563 [==============================] - 25s 44ms/step - loss: 0.4624 - val_loss: 0.5791\n",
      "Epoch 00036: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff48d84a58>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_on_batch,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_on_batch,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder model from the tensors we previously declared.\n",
    "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# Generate a new set of tensors for our new inference decoder. Note that we are using new tensors, \n",
    "# this does not preclude using the same underlying layers that we trained on. (e.g. weights/biases).\n",
    "inf_decoder_inputs = Input(shape=(None,), name=\"inf_decoder_inputs\")\n",
    "# We'll need to force feed the two state variables into the decoder each step.\n",
    "state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "decoder_res, decoder_h, decoder_c = decoder_lstm(\n",
    "    decoder_emb(inf_decoder_inputs), \n",
    "    initial_state=[state_input_h, state_input_c])\n",
    "inf_decoder_out = decoder_d2(decoder_d1(decoder_res))\n",
    "inf_model = Model(inputs=[inf_decoder_inputs, state_input_h, state_input_c], \n",
    "                  outputs=[inf_decoder_out, decoder_h, decoder_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_sentence, infenc_model, infmodel, attention=False):\n",
    "    print(\"[input sentence]\", input_sentence)\n",
    "    sv = tokenizer.EncodeAsIds(input_sentence)\n",
    "    sv = np.array(sv).reshape(1, len(sv)).tolist()\n",
    "    sv = pad_or_truncate_inputs(sv, max_enc_seq)\n",
    "    sv = np.array(sv)\n",
    "    # Reshape so we can use the encoder model. New shape=[samples,sequence length]\n",
    "    #sv = sv.reshape(1,len(sv))\n",
    "    [emb_out, sh, sc] = infenc_model.predict(x=sv)\n",
    "    \n",
    "    i = 0\n",
    "    start_vec = start_token_id\n",
    "    stop_vec = end_token_id\n",
    "    # We will continuously feed cur_vec as an input into the decoder to produce the next word,\n",
    "    # which will be assigned to cur_vec. Start it with \"<start>\".\n",
    "    cur_vec = np.zeros((1,1))\n",
    "    cur_vec[0,0] = start_vec\n",
    "    #cur_word = \"<start>\"\n",
    "    output_sentence = \"\"\n",
    "    output_sequence = []\n",
    "    len_target = max_dec_seq\n",
    "    # Start doing the feeding. Terminate when the model predicts an \"<end>\" or we reach the end\n",
    "    # of the max target language sentence length.\n",
    "    while cur_vec[0,0] != end_token_id and i < (len_target-1):\n",
    "        i += 1\n",
    "        #if cur_word != \"<start>\":\n",
    "        #    output_sentence = output_sentence + \" \" + cur_word\n",
    "        x_in = [cur_vec, sh, sc]\n",
    "        # This will allow us to accomodate attention models, which we will talk about later.\n",
    "        if attention:\n",
    "            x_in += [emb_out]\n",
    "        [nvec, sh, sc] = infmodel.predict(x=x_in)\n",
    "        # The output of the model is a massive softmax vector with one spot for every possible word. Convert\n",
    "        # it to a word ID using argmax().\n",
    "        cur_vec[0,0] = np.argmax(nvec[0,0])\n",
    "        output_sequence.append(int(cur_vec[0,0]))\n",
    "    \n",
    "    result = tokenizer.DecodeIds(output_sequence)\n",
    "    return \"[output sentence]\" + result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3418.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input sentence] This park reminds me of my childhood.\n",
      "[output sentence]その少年は私の兄を養うのが好きだ。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"This park reminds me of my childhood.\", encoder_model, inf_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_in_size = 8000\n",
    "vocab_out_size = 8000\n",
    "units = 256\n",
    "embedding_dim = 300\n",
    "len_input = max_enc_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'lstm_with_attention_1/transpose_1:0' shape=(?, ?, ?) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention_1/while/Exit_3:0' shape=(?, 256) dtype=float32>,\n",
       " <tf.Tensor 'lstm_with_attention_1/while/Exit_4:0' shape=(?, 256) dtype=float32>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RNN \"Cell\" classes in Keras perform the actual data transformations at each timestep. Therefore, in order\n",
    "# to add attention to LSTM, we need to make a custom subclass of LSTMCell.\n",
    "class AttentionLSTMCell(LSTMCell):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.attentionMode = False\n",
    "        super(AttentionLSTMCell, self).__init__(**kwargs)\n",
    "    \n",
    "    # Build is called to initialize the variables that our cell will use. We will let other Keras\n",
    "    # classes (e.g. \"Dense\") actually initialize these variables.\n",
    "    #@tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):        \n",
    "        # Converts the input sequence into a sequence which can be matched up to the internal\n",
    "        # hidden state.\n",
    "        self.dense_constant = TimeDistributed(Dense(self.units, name=\"AttLstmInternal_DenseConstant\"))\n",
    "        \n",
    "        # Transforms the internal hidden state into something that can be used by the attention\n",
    "        # mechanism.\n",
    "        self.dense_state = Dense(self.units, name=\"AttLstmInternal_DenseState\")\n",
    "        \n",
    "        # Transforms the combined hidden state and converted input sequence into a vector of\n",
    "        # probabilities for attention.\n",
    "        self.dense_transform = Dense(1, name=\"AttLstmInternal_DenseTransform\")\n",
    "        \n",
    "        # We will augment the input into LSTMCell by concatenating the context vector. Modify\n",
    "        # input_shape to reflect this.\n",
    "        batch, input_dim = input_shape[0]\n",
    "        batch, timesteps, context_size = input_shape[-1]\n",
    "        lstm_input = (batch, input_dim + context_size)\n",
    "        \n",
    "        # The LSTMCell superclass expects no constant input, so strip that out.\n",
    "        return super(AttentionLSTMCell, self).build(lstm_input)\n",
    "    \n",
    "    # This must be called before call(). The \"input sequence\" is the output from the \n",
    "    # encoder. This function will do some pre-processing on that sequence which will\n",
    "    # then be used in subsequent calls.\n",
    "    def setInputSequence(self, input_seq):\n",
    "        self.input_seq = input_seq\n",
    "        self.input_seq_shaped = self.dense_constant(input_seq)\n",
    "        self.timesteps = tf.shape(self.input_seq)[-2]\n",
    "    \n",
    "    # This is a utility method to adjust the output of this cell. When attention mode is\n",
    "    # turned on, the cell outputs attention probability vectors across the input sequence.\n",
    "    def setAttentionMode(self, mode_on=False):\n",
    "        self.attentionMode = mode_on\n",
    "    \n",
    "    # This method sets up the computational graph for the cell. It implements the actual logic\n",
    "    # that the model follows.\n",
    "    def call(self, inputs, states, constants):\n",
    "        # Separate the state list into the two discrete state vectors.\n",
    "        # ytm is the \"memory state\", stm is the \"carry state\".\n",
    "        ytm, stm = states\n",
    "        # We will use the \"carry state\" to guide the attention mechanism. Repeat it across all\n",
    "        # input timesteps to perform some calculations on it.\n",
    "        stm_repeated = K.repeat(self.dense_state(stm), self.timesteps)\n",
    "        # Now apply our \"dense_transform\" operation on the sum of our transformed \"carry state\" \n",
    "        # and all encoder states. This will squash the resultant sum down to a vector of size\n",
    "        # [batch,timesteps,1]\n",
    "        # Note: Most sources I encounter use tanh for the activation here. I have found with this dataset\n",
    "        # and this model, relu seems to perform better. It makes the attention mechanism far more crisp\n",
    "        # and produces better translation performance, especially with respect to proper sentence termination.\n",
    "        combined_stm_input = self.dense_transform(\n",
    "            keras.activations.relu(stm_repeated + self.input_seq_shaped))\n",
    "        # Performing a softmax generates a log probability for each encoder output to receive attention.\n",
    "        score_vector = keras.activations.softmax(combined_stm_input, 1)\n",
    "        # In this implementation, we grant \"partial attention\" to each encoder output based on \n",
    "        # it's log probability accumulated above. Other options would be to only give attention\n",
    "        # to the highest probability encoder output or some similar set.\n",
    "        context_vector = K.sum(score_vector * self.input_seq, 1)\n",
    "        \n",
    "        # Finally, mutate the input vector. It will now contain the traditional inputs (like the seq2seq\n",
    "        # we trained above) in addition to the attention context vector we calculated earlier in this method.\n",
    "        inputs = K.concatenate([inputs, context_vector])\n",
    "        \n",
    "        # Call into the super-class to invoke the LSTM math.\n",
    "        res = super(AttentionLSTMCell, self).call(inputs=inputs, states=states)\n",
    "        \n",
    "        # This if statement switches the return value of this method if \"attentionMode\" is turned on.\n",
    "        if(self.attentionMode):\n",
    "            return (K.reshape(score_vector, (-1, self.timesteps)), res[1])\n",
    "        else:\n",
    "            return res\n",
    "\n",
    "# Custom implementation of the Keras LSTM that adds an attention mechanism.\n",
    "# This is implemented by taking an additional input (using the \"constants\" of the\n",
    "# RNN class) into the LSTM: The encoder output vectors across the entire input sequence.\n",
    "class LSTMWithAttention(RNN):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        cell = AttentionLSTMCell(units=units)\n",
    "        self.units = units\n",
    "        super(LSTMWithAttention, self).__init__(cell, **kwargs)\n",
    "        \n",
    "    #@tf_utils.shape_type_conversion\n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[0][-1]\n",
    "        self.timesteps = input_shape[0][-2]\n",
    "        return super(LSTMWithAttention, self).build(input_shape) \n",
    "    \n",
    "    # This call is invoked with the entire time sequence. The RNN sub-class is responsible\n",
    "    # for breaking this up into calls into the cell for each step.\n",
    "    # The \"constants\" variable is the key to our implementation. It was specifically added\n",
    "    # to Keras to accomodate the \"attention\" mechanism we are implementing.\n",
    "    def call(self, x, constants, **kwargs):\n",
    "        if isinstance(x, list):\n",
    "            self.x_initial = x[0]\n",
    "        else:\n",
    "            self.x_initial = x\n",
    "        \n",
    "        # The only difference in the LSTM computational graph really comes from the custom\n",
    "        # LSTM Cell that we utilize.\n",
    "        self.cell._dropout_mask = None\n",
    "        self.cell._recurrent_dropout_mask = None\n",
    "        self.cell.setInputSequence(constants[0])\n",
    "        return super(LSTMWithAttention, self).call(inputs=x, constants=constants, **kwargs)\n",
    "\n",
    "# Below is test code to validate that this LSTM class and the associated cell create a\n",
    "# valid computational graph.\n",
    "test = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "test.cell.setAttentionMode(True)\n",
    "attenc_inputs2 = Input(shape=(len_input,))\n",
    "attenc_emb2 = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "test(inputs=attenc_emb2(attenc_inputs2), constants=attenc_emb2(attenc_inputs2), initial_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-create an entirely new model and set of layers for the attention model\n",
    "\n",
    "# Encoder Layers\n",
    "attenc_inputs = Input(shape=(len_input,), name=\"attenc_inputs\")\n",
    "attenc_emb = Embedding(input_dim=vocab_in_size, output_dim=embedding_dim)\n",
    "attenc_lstm = CuDNNLSTM(units=units, return_sequences=True, return_state=True)\n",
    "attenc_outputs, attstate_h, attstate_c = attenc_lstm(attenc_emb(attenc_inputs))\n",
    "attenc_states = [attstate_h, attstate_c]\n",
    "\n",
    "attdec_inputs = Input(shape=(None,))\n",
    "attdec_emb = Embedding(input_dim=vocab_out_size, output_dim=embedding_dim)\n",
    "attdec_lstm = LSTMWithAttention(units=units, return_sequences=True, return_state=True)\n",
    "# Note that the only real difference here is that we are feeding attenc_outputs to the decoder now.\n",
    "# Nice and clean!\n",
    "attdec_lstm_out, _, _ = attdec_lstm(inputs=attdec_emb(attdec_inputs), \n",
    "                                    constants=attenc_outputs, \n",
    "                                    initial_state=attenc_states)\n",
    "attdec_d1 = Dense(units, activation=\"relu\")\n",
    "attdec_d2 = Dense(vocab_out_size, activation=\"softmax\")\n",
    "attdec_out = attdec_d2(Dropout(rate=.4)(attdec_d1(Dropout(rate=.4)(attdec_lstm_out))))\n",
    "\n",
    "attmodel = Model([attenc_inputs, attdec_inputs], attdec_out)\n",
    "attmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", #metrics=['sparse_categorical_accuracy']\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "attenc_inputs (InputLayer)      (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 128, 300)     2400000     attenc_inputs[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, None, 300)    2400000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 128, 256), ( 571392      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_with_attention_2 (LSTMWith [(None, None, 256),  832512      embedding_3[0][0]                \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "                                                                 cu_dnnlstm_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, 256)    0           lstm_with_attention_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 256)    65792       dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 256)    0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, None, 8000)   2056000     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 8,325,696\n",
      "Trainable params: 8,325,696\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "attmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(attmodel, to_file=\"attention_nmt_model.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 161s 286ms/step - loss: 1.1735 - val_loss: 0.9291\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 155s 275ms/step - loss: 0.8986 - val_loss: 0.8496\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 155s 276ms/step - loss: 0.8281 - val_loss: 0.7839\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 154s 274ms/step - loss: 0.7725 - val_loss: 0.7381\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 157s 278ms/step - loss: 0.7313 - val_loss: 0.7075\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.6997 - val_loss: 0.6783\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 155s 276ms/step - loss: 0.6749 - val_loss: 0.6601\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 155s 276ms/step - loss: 0.6533 - val_loss: 0.6429\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 157s 279ms/step - loss: 0.6353 - val_loss: 0.6270\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 156s 278ms/step - loss: 0.6185 - val_loss: 0.6160\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 155s 276ms/step - loss: 0.6035 - val_loss: 0.6042\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - 155s 276ms/step - loss: 0.5888 - val_loss: 0.5938\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.5763 - val_loss: 0.5841\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.5638 - val_loss: 0.5764\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.5526 - val_loss: 0.5692\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.5413 - val_loss: 0.5619\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - 156s 276ms/step - loss: 0.5306 - val_loss: 0.5569\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - 157s 279ms/step - loss: 0.5201 - val_loss: 0.5520\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.5122 - val_loss: 0.5466\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - 157s 278ms/step - loss: 0.5027 - val_loss: 0.5412\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - 157s 279ms/step - loss: 0.4941 - val_loss: 0.5377\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - 158s 281ms/step - loss: 0.4856 - val_loss: 0.5347\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - 157s 279ms/step - loss: 0.4782 - val_loss: 0.5319\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.4715 - val_loss: 0.5295\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - 158s 280ms/step - loss: 0.4641 - val_loss: 0.5257\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.4565 - val_loss: 0.5264\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - 158s 281ms/step - loss: 0.4503 - val_loss: 0.5232\n",
      "Epoch 28/100\n",
      "563/563 [==============================] - 156s 278ms/step - loss: 0.4447 - val_loss: 0.5215\n",
      "Epoch 29/100\n",
      "563/563 [==============================] - 158s 280ms/step - loss: 0.4387 - val_loss: 0.5197\n",
      "Epoch 30/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.4336 - val_loss: 0.5181\n",
      "Epoch 31/100\n",
      "563/563 [==============================] - 156s 277ms/step - loss: 0.4282 - val_loss: 0.5177\n",
      "Epoch 32/100\n",
      "563/563 [==============================] - 154s 274ms/step - loss: 0.4218 - val_loss: 0.5155\n",
      "Epoch 33/100\n",
      "563/563 [==============================] - 155s 275ms/step - loss: 0.4168 - val_loss: 0.5171\n",
      "Epoch 34/100\n",
      "563/563 [==============================] - 156s 276ms/step - loss: 0.4112 - val_loss: 0.5181\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff50dab5c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 100\n",
    "#atthist = attmodel.fit([input_data, teacher_data], target_data,\n",
    "#                 batch_size=BATCH_SIZE,\n",
    "#                 epochs=epochs,\n",
    "#                 validation_split=0.2)\n",
    "#\n",
    "## Plot the results of the training.\n",
    "#plt.plot(atthist.history['sparse_categorical_accuracy'], label=\"Training loss\")\n",
    "#plt.plot(atthist.history['val_sparse_categorical_accuracy'], label=\"Validation loss\")\n",
    "#plt.show()\n",
    "\n",
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "\n",
    "attmodel.fit_generator(\n",
    "    generator=train_on_batch,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_on_batch,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1869.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input sentence] This park reminds me of my childhood.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[output sentence]車を修理するのは私のことを確信している。\n"
     ]
    }
   ],
   "source": [
    "def createAttentionInference(attention_mode=False):\n",
    "    # Create an inference model using the layers already trained above.\n",
    "    attencoder_model = Model(attenc_inputs, [attenc_outputs, attstate_h, attstate_c])\n",
    "    state_input_h = Input(shape=(units,), name=\"state_input_h\")\n",
    "    state_input_c = Input(shape=(units,), name=\"state_input_c\")\n",
    "    attenc_seq_out = Input(shape=attenc_outputs.get_shape()[1:], name=\"attenc_seq_out\")\n",
    "    inf_attdec_inputs = Input(shape=(None,), name=\"inf_attdec_inputs\")\n",
    "    attdec_lstm.cell.setAttentionMode(attention_mode)\n",
    "    attdec_res, attdec_h, attdec_c = attdec_lstm(attdec_emb(inf_attdec_inputs), \n",
    "                                                 initial_state=[state_input_h, state_input_c], \n",
    "                                                 constants=attenc_seq_out)\n",
    "    attinf_model = None\n",
    "    if not attention_mode:\n",
    "        inf_attdec_out = attdec_d2(attdec_d1(attdec_res))\n",
    "        attinf_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], \n",
    "                             outputs=[inf_attdec_out, attdec_h, attdec_c])\n",
    "    else:\n",
    "        attinf_model = Model(inputs=[inf_attdec_inputs, state_input_h, state_input_c, attenc_seq_out], \n",
    "                             outputs=[attdec_res, attdec_h, attdec_c])\n",
    "    return attencoder_model, attinf_model\n",
    "\n",
    "attencoder_model, attinf_model = createAttentionInference()\n",
    "print(translate(\"This park reminds me of my childhood.\", attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5924.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input sentence] I am feeling good today.\n",
      "[output sentence]今日は忙しい。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"I am feeling good today.\", attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 3738.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input sentence] Can I play tennis today?\n",
      "[output sentence]今日はテニスをする?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"Can I play tennis today?\", attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1401.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input sentence] I am feeling tired.\n",
      "[output sentence]私はとても疲れている。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"I am feeling tired.\", attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 5857.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input sentence] I want to eat some food now because I am so hungry.\n",
      "[output sentence]私はまだ泳ぐのは楽しみに行きたい。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(translate(\"I want to eat some food now because I am so hungry.\", attencoder_model, attinf_model, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 4169.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "お願いします。はいつもりです。はいつもりです。はいつもりだ。はいつもりだ。、私は私に言った。はいけない。、私は私\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Go.\"\n",
    "tokenized_ids = tokenizer.EncodeAsIds(input_text)\n",
    "tokenized_ids = np.array(tokenized_ids).reshape(1, len(tokenized_ids)).tolist()\n",
    "enc_input = pad_or_truncate_inputs(tokenized_ids, max_enc_seq)\n",
    "enc_input = np.array(enc_input).reshape(1, len(tokenized_ids[0]))\n",
    "_, sh, sc = encoder_model.predict(enc_input)\n",
    "output_seq = []\n",
    "cur_token = [start_token_id]\n",
    "#cur_token = np.array(cur_token).reshape(1, len(cur_token)).tolist()\n",
    "#cur_token = pad_or_truncate_inputs(cur_token, max_dec_seq)\n",
    "i = 0\n",
    "while cur_token != end_token_id and i < (max_dec_seq -1):\n",
    "    i += 1\n",
    "    #print(\"i\", cur_token)\n",
    "    #print(cur_token)\n",
    "    dec_inputs = [cur_token] + [sh, sc]\n",
    "    [out_dist_vec, sh, sc] = inf_model.predict(dec_inputs)\n",
    "    #print(out_dist_vec.shape)\n",
    "    output_token = np.argmax(out_dist_vec[0,0], axis=-1)\n",
    "    #print(\"o\", output_token)\n",
    "    output_seq.append(int(output_token))\n",
    "    cur_token = [output_token]\n",
    "    #cur_token = np.array(output_token).reshape(1, 1).tolist()\n",
    "    #cur_token = pad_or_truncate_inputs(cur_token, max_dec_seq)\n",
    "\n",
    "result = tokenizer.DecodeIds(output_seq)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_7/strided_slice_16:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'cu_dnnlstm_7/strided_slice_17:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"seq2seq_modified.h5\", include_optimizer=False)\n",
    "print(\"The model is saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"seq2seq_modified.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 128, 300)          2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     [(None, 256), (None, 256) 571392    \n",
      "=================================================================\n",
      "Total params: 2,971,392\n",
      "Trainable params: 2,971,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##inference network architecture\n",
    "enc_model = Model(enc_inputs, enc_states)\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"modified_enc_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7f681e7367d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdec_input_state_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdec_input_state_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_input_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdec_input_state_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input_state_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_input_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdec_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "dec_input_state_h = Input(shape=(hidden_dims,))\n",
    "dec_input_state_c = Input(shape=(hidden_dims,))\n",
    "dec_input_states = [dec_input_state_h, dec_input_state_c]\n",
    "dec_outputs, state_h, state_c = dec(dec_emb(dec_inputs), initial_state=dec_input_states)\n",
    "dec_states = [state_h, state_c]\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "dec_model = Model([dec_inputs] + dec_input_states,\n",
    "                 [dec_outputs] + dec_states)\n",
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"modified_dec_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1297.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "['私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"We listened to her for some time.\"\n",
    "tokenized_ids = tokenizer.EncodeAsIds(input_text)\n",
    "tokenized_ids = np.array(tokenized_ids).reshape(1, len(tokenized_ids)).tolist()\n",
    "enc_input = pad_or_truncate_inputs(tokenized_ids, max_enc_seq)\n",
    "enc_input = np.array(enc_input).reshape(1, len(tokenized_ids[0]))\n",
    "enc_states = enc_model.predict(enc_input)\n",
    "output_seq = []\n",
    "cur_token = [start_token_id]\n",
    "#cur_token = np.array(cur_token).reshape(1, len(cur_token)).tolist()\n",
    "#cur_token = pad_or_truncate_inputs(cur_token, max_dec_seq)\n",
    "i = 0\n",
    "while cur_token != end_token_id and i < (max_dec_seq -1):\n",
    "    i += 1\n",
    "    #print(\"i\", cur_token)\n",
    "    dec_inputs = [cur_token] + enc_states\n",
    "    [out_dist_vec, sh, sc] = dec_model.predict(dec_inputs)\n",
    "    print(out_dist_vec.shape)\n",
    "    output_token = np.argmax(out_dist_vec[0,0], axis=-1)\n",
    "    #print(\"o\", output_token)\n",
    "    output_seq.append(tokenizer.id_to_piece(int(output_token)))\n",
    "    cur_token = [output_token]\n",
    "    #cur_token = np.array(output_token).reshape(1, 1).tolist()\n",
    "    #cur_token = pad_or_truncate_inputs(cur_token, max_dec_seq)\n",
    "\n",
    "print(output_seq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
