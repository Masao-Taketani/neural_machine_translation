{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, Embedding, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "hidden_dims = 256\n",
    "\n",
    "data_file = \"jpn.txt\"\n",
    "enc_input_tokens = []\n",
    "dec_input_tokens = []\n",
    "dec_target_tokens = []\n",
    "start_token_id = 1\n",
    "end_token_id = 2\n",
    "pad_token_id = 3\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_list = f.read().split(\"\\n\")\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"sentencepiece/spm_for_nmt.model\")\n",
    "\n",
    "for line in lines_list:\n",
    "    #for the last black data, we need to skip\n",
    "    if line == \"\":\n",
    "        break\n",
    "    source_text, target_text = line.split(\"\\t\")\n",
    "    tokenized_source_text = tokenizer.EncodeAsPieces(source_text)\n",
    "    #test = tokenizer.EncodeAsIds(source_text)\n",
    "    #print(test)\n",
    "    tokenized_target_text = tokenizer.EncodeAsPieces(target_text)\n",
    "    \n",
    "    int_tokenized_source = []\n",
    "    int_tokenized_input_target = []\n",
    "    int_tokenized_output_target = []\n",
    "    for token in tokenized_source_text:\n",
    "        int_tokenized_source.append(tokenizer.piece_to_id(token))\n",
    "    for i, token in enumerate(tokenized_target_text):\n",
    "        if i == 0:\n",
    "            int_tokenized_input_target.append(start_token_id)\n",
    "            continue\n",
    "        int_tokenized_input_target.append(tokenizer.piece_to_id(token))\n",
    "        int_tokenized_output_target.append(tokenizer.piece_to_id(token))\n",
    "        \n",
    "    int_tokenized_output_target.append(end_token_id)\n",
    "    \n",
    "    if len(int_tokenized_output_target) != len(int_tokenized_input_target):\n",
    "        print(\"Error\")\n",
    "        \n",
    "    enc_input_tokens.append(int_tokenized_source)\n",
    "    dec_input_tokens.append(int_tokenized_input_target)\n",
    "    dec_target_tokens.append(int_tokenized_output_target)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_enc_seq = max([len(i) for i in enc_input_tokens])\n",
    "max_dec_seq = max([len(i) for i in dec_input_tokens])\n",
    "\n",
    "max_enc_seq, max_dec_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(pad_token_id)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45093/45093 [00:01<00:00, 44878.41it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 79013.37it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 79166.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_tokens = pad_or_truncate_inputs(enc_input_tokens, max_enc_seq)\n",
    "dec_input_tokens = pad_or_truncate_inputs(dec_input_tokens, max_dec_seq)\n",
    "dec_target_tokens = pad_or_truncate_inputs(dec_target_tokens, max_dec_seq)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1147,\n",
       " 523,\n",
       " 599,\n",
       " 395,\n",
       " 161,\n",
       " 107,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_tokens[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 168, 421, 6, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3] \n",
      " [168, 421, 6, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "print(dec_input_tokens[0],\"\\n\", dec_target_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def shuffle_dataset_and_split_into_train_test(enc_input, dec_input, dec_target, test_ratio=0.2):\n",
    "    dataset_list = list(zip(enc_input, dec_input, dec_target))\n",
    "    np.random.shuffle(dataset_list)\n",
    "    split_point = int(len(enc_input) * test_ratio)\n",
    "    test = dataset_list[:split_point]\n",
    "    train = dataset_list[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36075, 9018, 3, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = shuffle_dataset_and_split_into_train_test(enc_input_tokens, dec_input_tokens, dec_target_tokens)\n",
    "len(train), len(test), len(train[0]), len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61, 61)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0][0]), len(train[0][1]), len(train[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_list, batch_size, shuffle=False):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data_list)\n",
    "            \n",
    "        for i in range(0, len(data_list), batch_size):\n",
    "            enc_input_list = []\n",
    "            dec_input_list = []\n",
    "            dec_target_list = []\n",
    "            batch_list_inside_tuples = data_list[i: i + batch_size]\n",
    "            \n",
    "            for sample in batch_list_inside_tuples:\n",
    "                e_inp, d_inp, d_tar = sample[0], sample[1], sample[2]\n",
    "                enc_input_list.append(e_inp)\n",
    "                dec_input_list.append(d_inp)\n",
    "                dec_target_list.append(d_tar)\n",
    "            np_batch_enc_input = np.vstack(enc_input_list)\n",
    "            np_batch_dec_input = np.vstack(dec_input_list)\n",
    "            np_batch_dec_target = np.vstack(dec_target_list)\n",
    "            np_batch_dec_target_one_hot = to_categorical(np_batch_dec_target, num_classes=vocab_size)\n",
    "            ##input values are inside of [], and the rest is output value\n",
    "            yield [np_batch_enc_input, np_batch_dec_input], np_batch_dec_target_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 140)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_batch = generate_data(train, batch_size, shuffle=True)\n",
    "test_on_batch = generate_data(test, batch_size)\n",
    "train_steps_per_epoch = len(train) // batch_size\n",
    "test_steps_per_epoch = len(test) // batch_size\n",
    "train_steps_per_epoch, test_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 300)     2400000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 300)    2400000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 256), (None, 571392      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        [(None, None, 256),  571392      embedding_2[0][0]                \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 8000)   2056000     cu_dnnlstm_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,998,784\n",
      "Trainable params: 7,998,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "hidden_dims = 256\n",
    "emb_dims = 300\n",
    "\n",
    "##training network architecture\n",
    "enc_inputs = Input(shape=(max_enc_seq,))\n",
    "enc_emb = Embedding(vocab_size, emb_dims)(enc_inputs)\n",
    "enc = CuDNNLSTM(hidden_dims, return_state=True)\n",
    "_, state_h, state_c = enc(enc_emb)\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "dec_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(vocab_size, emb_dims)(dec_inputs)\n",
    "#return_state is used when the model inferences\n",
    "dec = CuDNNLSTM(hidden_dims, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec(dec_emb, initial_state=enc_states)\n",
    "dec_dense = Dense(8000, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"seq2seq_for_correct_format.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 56s 100ms/step - loss: 1.2054 - acc: 0.8357 - val_loss: 0.9294 - val_acc: 0.8509\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 50s 88ms/step - loss: 0.8705 - acc: 0.8576 - val_loss: 0.8368 - val_acc: 0.8616\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 44s 77ms/step - loss: 0.7911 - acc: 0.8667 - val_loss: 0.7759 - val_acc: 0.8686\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.7381 - acc: 0.8727 - val_loss: 0.7351 - val_acc: 0.8739\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.6951 - acc: 0.8773 - val_loss: 0.7078 - val_acc: 0.8770\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.6630 - acc: 0.8806 - val_loss: 0.6847 - val_acc: 0.8795\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.6341 - acc: 0.8839 - val_loss: 0.6683 - val_acc: 0.8818\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.6116 - acc: 0.8864 - val_loss: 0.6543 - val_acc: 0.8837\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 41s 74ms/step - loss: 0.5900 - acc: 0.8892 - val_loss: 0.6426 - val_acc: 0.8861\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.5710 - acc: 0.8917 - val_loss: 0.6345 - val_acc: 0.8871\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.5542 - acc: 0.8936 - val_loss: 0.6280 - val_acc: 0.8883\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.5388 - acc: 0.8956 - val_loss: 0.6220 - val_acc: 0.8893\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.5240 - acc: 0.8976 - val_loss: 0.6172 - val_acc: 0.8903\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.5112 - acc: 0.8993 - val_loss: 0.6149 - val_acc: 0.8909\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.4980 - acc: 0.9012 - val_loss: 0.6101 - val_acc: 0.8918\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.4853 - acc: 0.9033 - val_loss: 0.6030 - val_acc: 0.8938\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.4690 - acc: 0.9064 - val_loss: 0.5995 - val_acc: 0.8945\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.4572 - acc: 0.9082 - val_loss: 0.5975 - val_acc: 0.8953\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.4463 - acc: 0.9101 - val_loss: 0.5951 - val_acc: 0.8958\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - 41s 72ms/step - loss: 0.4360 - acc: 0.9116 - val_loss: 0.5918 - val_acc: 0.8967\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.4251 - acc: 0.9134 - val_loss: 0.5918 - val_acc: 0.8970\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - 41s 74ms/step - loss: 0.4167 - acc: 0.9149 - val_loss: 0.5897 - val_acc: 0.8977\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.4067 - acc: 0.9166 - val_loss: 0.5891 - val_acc: 0.8978\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - 42s 75ms/step - loss: 0.3979 - acc: 0.9182 - val_loss: 0.5896 - val_acc: 0.8980\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - 51s 90ms/step - loss: 0.3893 - acc: 0.9197 - val_loss: 0.5882 - val_acc: 0.8985\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - 51s 90ms/step - loss: 0.3804 - acc: 0.9214 - val_loss: 0.5890 - val_acc: 0.8988\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - 51s 90ms/step - loss: 0.3733 - acc: 0.9225 - val_loss: 0.5898 - val_acc: 0.8993\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff369216d8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=2, verbose=1, mode=\"auto\")\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_on_batch,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_on_batch,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_7/strided_slice_16:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'cu_dnnlstm_7/strided_slice_17:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is saved!\n"
     ]
    }
   ],
   "source": [
    "model.save(\"seq2seq_modified.h5\", include_optimizer=False)\n",
    "print(\"The model is saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"seq2seq_modified.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 128, 300)          2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     [(None, 256), (None, 256) 571392    \n",
      "=================================================================\n",
      "Total params: 2,971,392\n",
      "Trainable params: 2,971,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##inference network architecture\n",
    "enc_model = Model(enc_inputs, enc_states)\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"modified_enc_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Input' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7f681e7367d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdec_input_state_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdec_input_state_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_input_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdec_input_state_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input_state_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_input_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdec_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Input' is not defined"
     ]
    }
   ],
   "source": [
    "dec_input_state_h = Input(shape=(hidden_dims,))\n",
    "dec_input_state_c = Input(shape=(hidden_dims,))\n",
    "dec_input_states = [dec_input_state_h, dec_input_state_c]\n",
    "dec_outputs, state_h, state_c = dec(dec_emb(dec_inputs), initial_state=dec_input_states)\n",
    "dec_states = [state_h, state_c]\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "dec_model = Model([dec_inputs] + dec_input_states,\n",
    "                 [dec_outputs] + dec_states)\n",
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"modified_dec_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1297.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "(1, 1, 8000)\n",
      "['私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と', '私', 'も', '彼女', 'いない', 'と']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"We listened to her for some time.\"\n",
    "tokenized_ids = tokenizer.EncodeAsIds(input_text)\n",
    "tokenized_ids = np.array(tokenized_ids).reshape(1, len(tokenized_ids)).tolist()\n",
    "enc_input = pad_or_truncate_inputs(tokenized_ids, max_enc_seq)\n",
    "enc_input = np.array(enc_input).reshape(1, len(tokenized_ids[0]))\n",
    "enc_states = enc_model.predict(enc_input)\n",
    "output_seq = []\n",
    "cur_token = [start_token_id]\n",
    "#cur_token = np.array(cur_token).reshape(1, len(cur_token)).tolist()\n",
    "#cur_token = pad_or_truncate_inputs(cur_token, max_dec_seq)\n",
    "i = 0\n",
    "while cur_token != end_token_id and i < (max_dec_seq -1):\n",
    "    i += 1\n",
    "    #print(\"i\", cur_token)\n",
    "    dec_inputs = [cur_token] + enc_states\n",
    "    [out_dist_vec, sh, sc] = dec_model.predict(dec_inputs)\n",
    "    print(out_dist_vec.shape)\n",
    "    output_token = np.argmax(out_dist_vec[0,0], axis=-1)\n",
    "    #print(\"o\", output_token)\n",
    "    output_seq.append(tokenizer.id_to_piece(int(output_token)))\n",
    "    cur_token = [output_token]\n",
    "    #cur_token = np.array(output_token).reshape(1, 1).tolist()\n",
    "    #cur_token = pad_or_truncate_inputs(cur_token, max_dec_seq)\n",
    "\n",
    "print(output_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'閉策役副社長ーション本部 本部長 石渡 元春'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_enc = tokenizer.EncodeAsIds(input_text)\n",
    "test_dec = tokenizer.DecodeIds(test_enc)\n",
    "test_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 1, 2], [3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "l = [0, 1, 2, 3, 4, 5]\n",
    "\n",
    "print(np.array(l).reshape(-1, 3).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7, 76, 2282, 1404, 7, 1127, 99, 7, 5338, 3441]]\n"
     ]
    }
   ],
   "source": [
    "test = np.array(tokenized_ids).reshape(1, len(tokenized_ids)).tolist()\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'マ'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.id_to_piece(76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁', 'マ', 'ネー', 'ジャー', '▁', '森', '川', '▁', '卓', '也']"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.EncodeAsPieces('マネージャー 森川 卓也')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
