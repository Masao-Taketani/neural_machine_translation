{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I referred to the following webpages for the implementation.\n",
    "- Implementation of Transformer<br>\n",
    "https://qiita.com/halhorn/items/c91497522be27bde17ce<br>\n",
    "https://github.com/kpot/keras-transformer/tree/master/keras_transformer<br>\n",
    "https://github.com/Lsdefine/attention-is-all-you-need-keras<br>\n",
    "- Usage of \"\\_\\_call\\_\\_\" method<br>\n",
    "https://qiita.com/kyo-bad/items/439d8cc3a0424c45214a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding, Input, Reshape, Lambda, Add\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "d_model = 512\n",
    "MAX_LEN = 716\n",
    "class_num = 9\n",
    "PAD_ID = 3\n",
    "warmup_steps = 4000\n",
    "NUM_TRAIN = 5893\n",
    "NUM_TEST = 1474\n",
    "batch_size = 16\n",
    "epochs = 600\n",
    "negative_inf = -1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, max_len, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        def reshape(x):\n",
    "            x = tf.reshape(x, [-1, self.max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        out = Lambda(reshape)(x)\n",
    "        return out\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        def reshape(x):\n",
    "            heads = tf.transpose(x, [0, 2, 1, 3])\n",
    "            return tf.reshape(x, [-1, self.max_len, self.hidden_dim])\n",
    "        \n",
    "        out = Lambda(reshape)(heads)\n",
    "        return out\n",
    "        \n",
    "    def __call__(self, query, memory, attention_mask):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q = Lambda(lambda x: x * (depth_inside_each_head ** -0.5))(q)\n",
    "        \n",
    "        #q.shape = (batch_size, head_num, query_len, emb_dim)\n",
    "        #k.shape = (batch_size, head_num, memory_len, emb_dim)\n",
    "        #batch_dot(q, k).shape = (batch_size, head_num, query_len, memory_len)\n",
    "        score = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 3]))([q, k])\n",
    "        neg_inf_for_pads = Lambda(lambda x: K.cast_to_floatx(x) * negative_inf)(attention_mask)\n",
    "        masked_score = Add()([score, neg_inf_for_pads])\n",
    "        \n",
    "        normalized_score = Activation(\"softmax\")(masked_score)\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score)\n",
    "        \n",
    "        #normalized_score.shape = (batch_size, head_num, query_length, memory_length)\n",
    "        #v.shape = (batch_size, head_num, memory_length, depth)\n",
    "        #attention_weighted_output.shape = (batch_size, head_num, query_length, depth)\n",
    "        attention_weighted_output = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 2]))([normalized_score, v])\n",
    "        attention_weighted_output = self.combine_heads(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, query, attention_mask):\n",
    "        return super().__call__(query, query, attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\")\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"axis\"] = self.axis\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        mean = K.mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = K.var(inputs, axis=[-1], keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_inputs * self.scale + self.shift\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper():\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.layer(x)\n",
    "        outputs = self.dropout_layer(x)\n",
    "        results = Add()([inputs, outputs])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer): \n",
    "    def call(self, inputs):\n",
    "        data_type = inputs.dtype\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(inputs))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + Ï€/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return inputs + batched_positional_encoding\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeZeroPads(Layer):\n",
    "    def __init__(self, seq_len, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        self.emb_dim = emb_dim\n",
    "        super(MakeZeroPads, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mask_for_pads = tf.to_float(tf.not_equal(inputs, PAD_ID))\n",
    "        pads_masked_embedding = inputs * mask_for_pads\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, vocab_size, max_len, stack_num, head_num, emb_dim, dropout_rate, *args, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding_layer = Embedding(self.vocab_size,\n",
    "                           self.emb_dim,\n",
    "                           embeddings_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                          )\n",
    "        self.make_zero_pads_layer = MakeZeroPads(self.max_len, vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(self.max_len, self.emb_dim, self.head_num, self.dropout_rate)\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(self.emb_dim, self.dropout_rate)\n",
    "            self.attention_block_list.append([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate)\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "    def __call__(self, inputs, self_attention_mask):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.make_zero_pads_layer(x)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        x = self.input_dropout_layer(x)\n",
    "        \n",
    "        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "            x = self_attention_layer(query=x, attention_mask=self_attention_mask)\n",
    "            x = pffn_layer(x)\n",
    "            \n",
    "        return self.output_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder():\n",
    "    def __init__(self, vocab_size, stack_num, head_num, emb_dim, dropout_rate, max_len, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding_layer = Embedding(self.vocab_size,\n",
    "                   self.emb_dim,\n",
    "                   embeddings_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                  )\n",
    "        self.make_zero_pads_layer = MakeZeroPads(self.max_len, vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(self.max_len, self.emb_dim, self.head_num, self.dropout_rate)\n",
    "            sou_tar_attn_layer = MultiheadAttention(self.max_len, self.emb_dim, self.head_num, self.dropout_rate)\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(self.emb_dim, self.dropout_rate)\n",
    "            self.attention_block_list.append([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate),\n",
    "                PreLayerNormPostResidualConnectionWrapper(sou_tar_attn_layer, dropout_rate),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate)\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "        ## need to change output dense into shared emb weight\n",
    "        self.output_dense_layer = Dense(vocab_size, use_bias=False)\n",
    "        \n",
    "    def __call__(self, inputs, encoder_outputs, self_attention_mask, sou_tar_attn_mask, train_flag):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.make_zero_pads_layer(x)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        x = self.input_dropout_layer(x)\n",
    "        \n",
    "        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, sou_tar_attn_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "            x = self_attention_layer(query=x, attention_mask=self_attention_mask)\n",
    "            x = sou_tar_attn_layer(query=x, memory=encoder_output, attention_mask=sou_tar_attn_mask)\n",
    "            x = pffn_layer(x)\n",
    "            \n",
    "        x = self.output_layer_norm(x)\n",
    "        return self.output_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer():\n",
    "    def __init__(self, vocab_size, stack_num, head_num, emb_dim, dropout_rate, max_len, *args, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.encoder = Encoder(vocab_size=vocab_size,\n",
    "                    stack_num=stack_num,\n",
    "                    head_num=head_num,\n",
    "                    emb_dim=emb_dim,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    max_len=max_len\n",
    "                              )\n",
    "        self.decoder = Decoder(vocab_size=vocab_size,\n",
    "                              stack_num=stack_num,\n",
    "                              head_num=head_num,\n",
    "                              emb_dim=emb_dim,\n",
    "                              dropout_rate=dropout_rate,\n",
    "                              max_len=max_len\n",
    "                              )\n",
    "    # Mask for pads\n",
    "    def create_enc_attention_mask(self, encoder_input):\n",
    "        batch_size, length = tf.unstack(tf.shape(encoder_input))\n",
    "        pad_array = tf.equal(encoder_input, PAD_ID)\n",
    "        pad_array = tf.reshape(pad_array, [batch_size, 1, 1, length])\n",
    "        \n",
    "    # Mask for pads and tokens that are located at greater time step than current time step\n",
    "    def create_dec_self_attention_mask(self, decoder_input):\n",
    "        batch_size, length = tf.unstack(tf.shape(decpder_input))\n",
    "        pad_array = tf.equal(decoder_input, PAD_ID)\n",
    "        pad_array = tf.reshape(pad_array, [batch_size, 1, 1, length])\n",
    "        \n",
    "        autoregression_array = tf.logical_not(\n",
    "            tf.matrix_band_part(tf.ones([length, length], dtype=tf.bool), -1, 0)\n",
    "        )\n",
    "        autoregression_array = tf.reshape(autpregression_array, [1, 1, length, length])\n",
    "        return tf.logical_or(pad_array, autoregression_array)\n",
    "        \n",
    "    def __call__(self, encoder_input, decoder_input):\n",
    "        enc_attention_mask = self.create_enc_attention_mask(encoder_input)\n",
    "        dec_self_attention_mask = self.create_dec_self_attention_mask(decoder_input)\n",
    "        \n",
    "        encoder_output = self.encoder(\n",
    "            encoder_input,\n",
    "            self_attention_mask=enc_attention_mask\n",
    "        )\n",
    "        \n",
    "        decoder_output = self.decoder(\n",
    "            decoder_input,\n",
    "            self_encoder_output,\n",
    "            self_attention_mask=dec_self_attention_mask,\n",
    "            sou_tar_attn_mask=enc_attention_mask\n",
    "        )\n",
    "        \n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "data_file = \"jpn.txt\"\n",
    "\n",
    "enc_input_tokens = []\n",
    "dec_input_tokens = []\n",
    "dec_target_tokens = []\n",
    "start_token_id = 1\n",
    "end_token_id = 2\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_list = f.read().split(\"\\n\")\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"sentencepiece/spm_for_nmt.model\")\n",
    "\n",
    "for line in lines_list:\n",
    "    #for the last blank data, we need to skip\n",
    "    if line == \"\":\n",
    "        break\n",
    "    source_text, target_text = line.split(\"\\t\")\n",
    "    tokenized_source_text = tokenizer.EncodeAsPieces(source_text)\n",
    "    #test = tokenizer.EncodeAsIds(source_text)\n",
    "    #print(test)\n",
    "    tokenized_target_text = tokenizer.EncodeAsPieces(target_text)\n",
    "    \n",
    "    int_tokenized_source = []\n",
    "    int_tokenized_input_target = []\n",
    "    int_tokenized_output_target = []\n",
    "    for token in tokenized_source_text:\n",
    "        int_tokenized_source.append(tokenizer.piece_to_id(token))\n",
    "    for i, token in enumerate(tokenized_target_text):\n",
    "        if i == 0:\n",
    "            int_tokenized_input_target.append(start_token_id)\n",
    "            continue\n",
    "        int_tokenized_input_target.append(tokenizer.piece_to_id(token))\n",
    "        int_tokenized_output_target.append(tokenizer.piece_to_id(token))\n",
    "        \n",
    "    int_tokenized_output_target.append(end_token_id)\n",
    "    \n",
    "    if len(int_tokenized_output_target) != len(int_tokenized_input_target):\n",
    "        print(\"Error\")\n",
    "        \n",
    "    enc_input_tokens.append(int_tokenized_source)\n",
    "    dec_input_tokens.append(int_tokenized_input_target)\n",
    "    dec_target_tokens.append(int_tokenized_output_target)\n",
    "    \n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_enc_seq = max([len(i) for i in enc_input_tokens])\n",
    "max_dec_seq = max([len(i) for i in dec_input_tokens])\n",
    "\n",
    "max_enc_seq, max_dec_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(PAD_ID)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_input_tokens = pad_or_truncate_inputs(enc_input_tokens, max_enc_seq)\n",
    "dec_input_tokens = pad_or_truncate_inputs(dec_input_tokens, max_dec_seq)\n",
    "dec_target_tokens = pad_or_truncate_inputs(dec_target_tokens, max_dec_seq)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def shuffle_dataset_and_split_into_train_test(enc_input, dec_input, dec_target, test_ratio=0.2):\n",
    "    dataset_list = list(zip(enc_input, dec_input, dec_target))\n",
    "    np.random.shuffle(dataset_list)\n",
    "    split_point = int(len(enc_input) * test_ratio)\n",
    "    test = dataset_list[:split_point]\n",
    "    train = dataset_list[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = shuffle_dataset_and_split_into_train_test(enc_input_tokens, dec_input_tokens, dec_target_tokens)\n",
    "len(train), len(test), len(train[0]), len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train[0][0]), len(train[0][1]), len(train[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_list, batch_size, shuffle=False):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data_list)\n",
    "            \n",
    "        for i in range(0, len(data_list), batch_size):\n",
    "            enc_input_list = []\n",
    "            dec_input_list = []\n",
    "            dec_target_list = []\n",
    "            batch_list_inside_tuples = data_list[i: i + batch_size]\n",
    "            \n",
    "            for sample in batch_list_inside_tuples:\n",
    "                e_inp, d_inp, d_tar = sample[0], sample[1], sample[2]\n",
    "                enc_input_list.append(e_inp)\n",
    "                dec_input_list.append(d_inp)\n",
    "                dec_target_list.append(d_tar)\n",
    "            np_batch_enc_input = np.vstack(enc_input_list)\n",
    "            np_batch_dec_input = np.vstack(dec_input_list)\n",
    "            np_batch_dec_target = np.vstack(dec_target_list)\n",
    "            np_batch_dec_target_one_hot = to_categorical(np_batch_dec_target, num_classes=vocab_size)\n",
    "            ##input values are inside of [], and the rest is output value\n",
    "            yield [np_batch_enc_input, np_batch_dec_input], np_batch_dec_target_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_on_batch = generate_data(train, batch_size, shuffle=True)\n",
    "test_on_batch = generate_data(test, batch_size)\n",
    "train_steps_per_epoch = len(train) // batch_size\n",
    "test_steps_per_epoch = len(test) // batch_size\n",
    "train_steps_per_epoch, test_steps_per_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## need to customize below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##training network architecture\n",
    "enc_inputs = Input(shape=(max_enc_seq,))\n",
    "enc_emb = Embedding(vocab_size, emb_dims)(enc_inputs)\n",
    "enc = CuDNNLSTM(hidden_dims, return_state=True)\n",
    "_, state_h, state_c = enc(enc_emb)\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "dec_inputs = Input(shape=(max_dec_seq,))\n",
    "dec_emb = Embedding(vocab_size, emb_dims)(dec_inputs)\n",
    "#return_state is used when the model inferences\n",
    "dec = CuDNNLSTM(hidden_dims, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec(dec_emb, initial_state=enc_states)\n",
    "dec_dense = Dense(8000, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"seq2seq.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=1, verbose=1, mode=\"auto\")\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_on_batch,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_on_batch,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45093\n"
     ]
    }
   ],
   "source": [
    "dataset = list(zip(enc_input_tokens, dec_input_tokens, dec_target_tokens))\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class BatchGenerator():\n",
    "    def __init__(self, max_len=MAX_LEN):\n",
    "        self.max_len = max_len\n",
    "        self.bos_id = 1\n",
    "        self.eos_id = 2\n",
    "        self.pad_id = 3\n",
    "        \n",
    "    def split(self, data_list, batch_size):\n",
    "        return [(data_list[i - batch_size: i]) for i in range(batch_size, len(data_list) + 1, batch_size)]\n",
    "        \n",
    "    def pad_and_convert_to_np_array(self, id_list_list):\n",
    "        #check the max length for each batch, and pad each sentence having the max length within a batch.\n",
    "        max_len = max([len(id_list) for id_list in id_list_list])\n",
    "        return np.array(\n",
    "            [(id_list) + [self.pad_id] * (max_len - len(id_list)) for id_list in id_list_list]\n",
    "        )\n",
    "        \n",
    "    def get_batch(self,dataset, batch_size, shuffle=True):\n",
    "        while True:\n",
    "            if shuffle:\n",
    "                random.shuffle(dataset)\n",
    "            raw_batch_list = self.split(dataset, batch_size)\n",
    "            test = self.pad_and_convert_to_np_array(raw_batch_list)\n",
    "            print(test.shape)\n",
    "            print(test[0][0])\n",
    "            break\n",
    "            for raw_batch in raw_batch_list:\n",
    "                eng, jpn = zip(*raw_batch)\n",
    "                yield {\n",
    "                    #### need to implement\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2818, 16, 3)\n",
      "[list([1147, 2164, 136, 7, 3178, 2632, 498, 7, 287, 2363, 7, 3998, 3223, 7, 5092, 7, 545, 532, 7, 2327, 1253, 269, 7, 269, 658, 7, 2774, 3178, 2367, 161, 2608])\n",
      " list([1, 45, 81, 4, 71, 2096, 15, 2803, 24, 5928, 147, 67, 55, 314, 208, 2608])\n",
      " list([45, 81, 4, 71, 2096, 15, 2803, 24, 5928, 147, 67, 55, 314, 208, 2608, 2])]\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-6f6a37ad244b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch_gen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_gen = BatchGenerator()\n",
    "gen = batch_gen.get_batch(dataset, batch_size)\n",
    "tmp = next(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([7,\n",
       "   1946,\n",
       "   287,\n",
       "   7,\n",
       "   707,\n",
       "   2976,\n",
       "   7,\n",
       "   2056,\n",
       "   2292,\n",
       "   7,\n",
       "   1372,\n",
       "   2566,\n",
       "   7,\n",
       "   437,\n",
       "   287,\n",
       "   7,\n",
       "   486,\n",
       "   567,\n",
       "   107],\n",
       "  [1, 1543, 8, 148, 200, 227, 15, 3496, 6],\n",
       "  [1543, 8, 148, 200, 227, 15, 3496, 6, 2]),\n",
       " ([3175,\n",
       "   7,\n",
       "   2327,\n",
       "   1253,\n",
       "   269,\n",
       "   7,\n",
       "   498,\n",
       "   2963,\n",
       "   369,\n",
       "   1190,\n",
       "   269,\n",
       "   7,\n",
       "   437,\n",
       "   1338,\n",
       "   3386,\n",
       "   4407,\n",
       "   7,\n",
       "   707,\n",
       "   523,\n",
       "   1056,\n",
       "   1147,\n",
       "   2164,\n",
       "   7,\n",
       "   161,\n",
       "   217,\n",
       "   2963,\n",
       "   107],\n",
       "  [1, 1137, 8, 3618, 45, 81, 5, 573, 106, 587, 380, 310, 494, 864, 407, 6],\n",
       "  [1137, 8, 3618, 45, 81, 5, 573, 106, 587, 380, 310, 494, 864, 407, 6, 2])]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\"one\":\"a\", \"two\":\"b\", \"three\": \"c\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar(**kwargs):\n",
    "    for key in kwargs:\n",
    "        print(\"key:\", key, \" value:\", kwargs[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key: one  value: a\n",
      "key: three  value: c\n",
      "key: two  value: b\n"
     ]
    }
   ],
   "source": [
    "bar(**test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
