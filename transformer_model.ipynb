{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I referred to [**the webpage**](https://qiita.com/halhorn/items/c91497522be27bde17ce) for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding, Input, Reshape, Lambda, Add\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "d_model = 512\n",
    "MAX_LEN = 716\n",
    "class_num = 9\n",
    "PAD_ID = 0\n",
    "warmup_steps = 4000\n",
    "NUM_TRAIN = 5893\n",
    "NUM_TEST = 1474\n",
    "batch_size = 16\n",
    "epochs = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, max_len, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        def reshape(x):\n",
    "            x = tf.reshape(x, [-1, self.max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        out = Lambda(reshape)(x)\n",
    "        return out\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        def reshape(x):\n",
    "            heads = tf.transpose(x, [0, 2, 1, 3])\n",
    "            return tf.reshape(x, [-1, self.max_len, self.hidden_dim])\n",
    "        \n",
    "        out = Lambda(reshape)(heads)\n",
    "        return out\n",
    "        \n",
    "    def __call__(self, query, memory):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q = Lambda(lambda x: x * (depth_inside_each_head ** -0.5))(q)\n",
    "        \n",
    "        #q.shape = (batch_size, head_num, query_len, emb_dim)\n",
    "        #k.shape = (batch_size, head_num, memory_len, emb_dim)\n",
    "        #batch_dot(q, k).shape = (batch_size, head_num, query_len, memory_len)\n",
    "        score = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 3]))([q, k])\n",
    "        normalized_score = Activation(\"softmax\")(score)\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score)\n",
    "        \n",
    "        #normalized_score.shape = (batch_size, head_num, query_length, memory_length)\n",
    "        #v.shape = (batch_size, head_num, memory_length, depth)\n",
    "        #attention_weighted_output.shape = (batch_size, head_num, query_length, depth)\n",
    "        attention_weighted_output = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 2]))([normalized_score, v])\n",
    "        attention_weighted_output = self.combine_heads(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        return super().__call__(query, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\")\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"axis\"] = self.axis\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        mean = K.mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = K.var(inputs, axis=[-1], keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_inputs * self.scale + self.shift\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper():\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.layer(x)\n",
    "        outputs = self.dropout_layer(x)\n",
    "        results = Add()([inputs, outputs])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer): \n",
    "    def call(self, inputs):\n",
    "        data_type = inputs.dtype\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(inputs))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + Ï€/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return inputs + batched_positional_encoding\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeZeroPads(Layer):\n",
    "    def __init__(self, seq_len, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        self.emb_dim = emb_dim\n",
    "        super(MakeZeroPads, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mask_for_pads = tf.to_float(tf.not_equal(inputs, PAD_ID))\n",
    "        pads_masked_embedding = inputs * mask_for_pads\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self, vodab_size, stack_num, head_num, emb_dim, dropout_rate, max_len, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_emb_layer = TokenEmbedding(vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(emb_dim, head_num, dropout_rate, name=\"self_attention_layer\")\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(emb_dim, dropout_rate, \"pffn_layer\")\n",
    "            self.attention_block_list.appen([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate, name=\"prepos_self_attention_wrapper\"),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate, name=\"prepos_pffn_wrapper\")\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "    def call(self, input, self_attention_mask, train_flag):\n",
    "        x = self.token_emb_layer(input)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        x = self.input_dropout_layer(x, training=train_flag)\n",
    "        \n",
    "        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "            x = self_attention_layer(x, attention_mask=self_attention_mask, train_flag=train_flag)\n",
    "            x = pffn_layer(x, train_flag=train_flag)\n",
    "            \n",
    "        return self.output_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
