{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I referred to [**the webpage**](https://qiita.com/halhorn/items/c91497522be27bde17ce) for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Model):\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False, name=\"q_dense_layer\")\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False, name=\"k_dense_layer\")\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False, name=\"v_dense_layer\")\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False, name=\"output_dense_layer\")\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate, name=\"attention_dropout_layer\")\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, max_len, hidden_dim = x.shape\n",
    "        x = x.reshape(batch_size, max_len, self.head_num, self.hidden_dim//self.head_num)\n",
    "        return x.transpose(0, 2, 1, 3)\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        batch_size, _, max_len, _ = heads.shape\n",
    "        heads = heads.transpose(0, 2, 1, 3)\n",
    "        return heads.reshape(batch_size, max_len, self.hidden_dim)\n",
    "        \n",
    "    def call(self, query, memory, attention_mask, train_flag):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q *= depth_inside_each_head ** -0.5\n",
    "        \n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        # query.dtype.min ≈ -∞\n",
    "        score += tf.to_float(attention_mask) * query.dtype.min\n",
    "        normalized_score = Activation(\"softmax\")(score, name=\"attention_weight\")\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score, training=train_flag)\n",
    "        attention_weighted_output = tf.matmul(normalized_score, v)\n",
    "        attention_weighted_output = self.combine_head(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def call(self, query, attention_mask, train_flag):\n",
    "        return super().call(query, query, attention_mask, train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardNetwork(Model):\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\", name=\"first_dense_layer\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\", name=\"second_dense_layer\")\n",
    "        self.dropout_layer = Dropout(dropout_rate, name=\"PFFN_dropout\")\n",
    "        \n",
    "    def call(self, input, train_flag):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(input)\n",
    "        x = self.dropout_layer(x, training=train_flag)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input, epsilon=1e-6):\n",
    "        mean = K.mean(input, axis=[-1], keepdims=True)\n",
    "        variance = K.var(input, axis=[-1], keepdims=True)\n",
    "        normalized_input = (input - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_input * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper(Model):\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, input, train_flag, *args, **kwargs):\n",
    "        x = self.layer_norm(input)\n",
    "        x = self.layer(x, training=train_flag, *args, **kwargs)\n",
    "        output = self.dropout_layer(x, training=train_flag)\n",
    "        return input + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer):\n",
    "    def call(self, input):\n",
    "        data_type = input.dtype\n",
    "        batch_size, max_len, emb_dim = input.shape\n",
    "        emb_dim_counter = list(range(emb_dim)) // 2\n",
    "        tmp1 = K.tile(K.expand_dims(emb_dim_counter, 0), [max_len, 1])\n",
    "        tmp2 = K.pow(10000.0, K.cast(tmp1 / emb_dim, data_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = list(range(5))\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.arange(40)\n",
    "print(test.shape)\n",
    "test = test.reshape(5,4,2)\n",
    "test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 0],\n",
       "         [ 1],\n",
       "         [ 2],\n",
       "         [ 3]],\n",
       "\n",
       "        [[ 4],\n",
       "         [ 5],\n",
       "         [ 6],\n",
       "         [ 7]]],\n",
       "\n",
       "\n",
       "       [[[ 8],\n",
       "         [ 9],\n",
       "         [10],\n",
       "         [11]],\n",
       "\n",
       "        [[12],\n",
       "         [13],\n",
       "         [14],\n",
       "         [15]]],\n",
       "\n",
       "\n",
       "       [[[16],\n",
       "         [17],\n",
       "         [18],\n",
       "         [19]],\n",
       "\n",
       "        [[20],\n",
       "         [21],\n",
       "         [22],\n",
       "         [23]]],\n",
       "\n",
       "\n",
       "       [[[24],\n",
       "         [25],\n",
       "         [26],\n",
       "         [27]],\n",
       "\n",
       "        [[28],\n",
       "         [29],\n",
       "         [30],\n",
       "         [31]]],\n",
       "\n",
       "\n",
       "       [[[32],\n",
       "         [33],\n",
       "         [34],\n",
       "         [35]],\n",
       "\n",
       "        [[36],\n",
       "         [37],\n",
       "         [38],\n",
       "         [39]]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.reshape(5, 2, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  2,  4,  6],\n",
       "        [ 8, 10, 12, 14],\n",
       "        [16, 18, 20, 22],\n",
       "        [24, 26, 28, 30],\n",
       "        [32, 34, 36, 38]],\n",
       "\n",
       "       [[ 1,  3,  5,  7],\n",
       "        [ 9, 11, 13, 15],\n",
       "        [17, 19, 21, 23],\n",
       "        [25, 27, 29, 31],\n",
       "        [33, 35, 37, 39]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
