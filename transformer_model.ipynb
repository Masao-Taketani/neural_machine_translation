{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I referred to [**the webpage**](https://qiita.com/halhorn/items/c91497522be27bde17ce) for the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(Model):\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False, name=\"q_dense_layer\")\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False, name=\"k_dense_layer\")\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False, name=\"v_dense_layer\")\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False, name=\"output_dense_layer\")\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate, name=\"attention_dropout_layer\")\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, max_len, hidden_dim = tf.unstack(tf.shape(x))\n",
    "        x = tf.reshape(x, [batch_size, max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "        return tf.transpose(x, [0, 2, 1, 3])\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        batch_size, _, max_len, _ = tf.unstack(tf.shape(heads))\n",
    "        heads = tf.transpose(heads, [0, 2, 1, 3])\n",
    "        return tf.reshape(heads, [batch_size, max_len, self.hidden_dim])\n",
    "        \n",
    "    def call(self, query, memory, attention_mask, train_flag):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q *= depth_inside_each_head ** -0.5\n",
    "        \n",
    "        score = tf.matmul(q, k, transpose_b=True)\n",
    "        # query.dtype.min ≈ -∞\n",
    "        score += tf.to_float(attention_mask) * query.dtype.min\n",
    "        normalized_score = Activation(\"softmax\")(score, name=\"attention_weight\")\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score, training=train_flag)\n",
    "        attention_weighted_output = tf.matmul(normalized_score, v)\n",
    "        attention_weighted_output = self.combine_head(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def call(self, query, attention_mask, train_flag):\n",
    "        return super().call(query, query, attention_mask, train_flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedforwardNetwork(Model):\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\", name=\"first_dense_layer\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\", name=\"second_dense_layer\")\n",
    "        self.dropout_layer = Dropout(dropout_rate, name=\"PFFN_dropout\")\n",
    "        \n",
    "    def call(self, input, train_flag):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(input)\n",
    "        x = self.dropout_layer(x, training=train_flag)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input, epsilon=1e-6):\n",
    "        mean = K.mean(input, axis=[-1], keepdims=True)\n",
    "        variance = K.var(input, axis=[-1], keepdims=True)\n",
    "        normalized_input = (input - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_input * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper(Model):\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def call(self, input, train_flag, *args, **kwargs):\n",
    "        x = self.layer_norm(input)\n",
    "        x = self.layer(x, training=train_flag, *args, **kwargs)\n",
    "        output = self.dropout_layer(x, training=train_flag)\n",
    "        return input + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer):\n",
    "    def call(self, input):\n",
    "        data_type = input.dtype\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(input))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + π/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return input + batched_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenEmbedding(Layer):\n",
    "    def __init__(self, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.data_type = data_type\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding_layer = Embedding(vocab_size,\n",
    "                                   emb_dim,\n",
    "                                   embedding_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                                  )\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, input):\n",
    "        mask_for_pad = tf.to_float(tf.not_equal(input, PAD_ID))\n",
    "        embedding = self.embedding_layer(input)\n",
    "        pads_masked_embedding *= tf.expand_dims(mask, -1)\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
