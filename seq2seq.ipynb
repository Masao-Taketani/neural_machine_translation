{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, Embedding, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "hidden_dims = 256\n",
    "\n",
    "data_file = \"jpn.txt\"\n",
    "enc_input_tokens = []\n",
    "dec_input_tokens = []\n",
    "dec_target_tokens = []\n",
    "start_token_id = 1\n",
    "end_token_id = 2\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_list = f.read().split(\"\\n\")\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"spm.model\")\n",
    "\n",
    "for line in lines_list:\n",
    "    #for the last black data, we need to skip\n",
    "    if line == \"\":\n",
    "        break\n",
    "    source_text, target_text = line.split(\"\\t\")\n",
    "    tokenized_source_text = tokenizer.EncodeAsPieces(source_text)\n",
    "    #test = tokenizer.EncodeAsIds(source_text)\n",
    "    #print(test)\n",
    "    tokenized_target_text = tokenizer.EncodeAsPieces(target_text)\n",
    "    \n",
    "    int_tokenized_source = []\n",
    "    int_tokenized_input_target = []\n",
    "    int_tokenized_output_target = []\n",
    "    for token in tokenized_source_text:\n",
    "        int_tokenized_source.append(tokenizer.piece_to_id(token))\n",
    "    for i, token in enumerate(tokenized_target_text):\n",
    "        if i == 0:\n",
    "            int_tokenized_input_target.append(start_token_id)\n",
    "            continue\n",
    "        int_tokenized_input_target.append(tokenizer.piece_to_id(token))\n",
    "        int_tokenized_output_target.append(tokenizer.piece_to_id(token))\n",
    "        \n",
    "    int_tokenized_output_target.append(end_token_id)\n",
    "    \n",
    "    if len(int_tokenized_output_target) != len(int_tokenized_input_target):\n",
    "        print(\"Error\")\n",
    "        \n",
    "    enc_input_tokens.append(int_tokenized_source)\n",
    "    dec_input_tokens.append(int_tokenized_input_target)\n",
    "    dec_target_tokens.append(int_tokenized_output_target)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_enc_seq = max([len(i) for i in enc_input_tokens])\n",
    "max_dec_seq = max([len(i) for i in dec_input_tokens])\n",
    "\n",
    "max_enc_seq, max_dec_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "    pad_id = 0\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(pad_id)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45093/45093 [00:00<00:00, 46974.08it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 86748.65it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 87255.44it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_tokens = pad_or_truncate_inputs(enc_input_tokens, max_enc_seq)\n",
    "dec_input_tokens = pad_or_truncate_inputs(dec_input_tokens, max_dec_seq)\n",
    "dec_target_tokens = pad_or_truncate_inputs(dec_target_tokens, max_dec_seq)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def shuffle_dataset_and_split_into_train_test(enc_input, dec_input, dec_target, test_ratio=0.2):\n",
    "    dataset_list = list(zip(enc_input, dec_input, dec_target))\n",
    "    np.random.shuffle(dataset_list)\n",
    "    split_point = int(len(enc_input) * test_ratio)\n",
    "    test = dataset_list[:split_point]\n",
    "    train = dataset_list[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36075, 9018, 3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = shuffle_dataset_and_split_into_train_test(enc_input_tokens, dec_input_tokens, dec_target_tokens)\n",
    "len(train), len(test), len(train[0]), len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61, 61)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0][0]), len(train[0][1]), len(train[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_list, batch_size, shuffle=False):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data_list)\n",
    "            \n",
    "        for i in range(0, len(data_list), batch_size):\n",
    "            enc_input_list = []\n",
    "            dec_input_list = []\n",
    "            dec_target_list = []\n",
    "            batch_list_inside_tuples = data_list[i: i + batch_size]\n",
    "            \n",
    "            for sample in batch_list_inside_tuples:\n",
    "                e_inp, d_inp, d_tar = sample[0], sample[1], sample[2]\n",
    "                enc_input_list.append(e_inp)\n",
    "                dec_input_list.append(d_inp)\n",
    "                dec_target_list.append(d_tar)\n",
    "            np_batch_enc_input = np.vstack(enc_input_list)\n",
    "            np_batch_dec_input = np.vstack(dec_input_list)\n",
    "            np_batch_dec_target = np.vstack(dec_target_list)\n",
    "            np_batch_dec_target_one_hot = to_categorical(np_batch_dec_target, num_classes=vocab_size)\n",
    "            ##input values are inside of [], and the rest is output value\n",
    "            yield [np_batch_enc_input, np_batch_dec_input], np_batch_dec_target_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 140)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_batch = generate_data(train, batch_size, shuffle=True)\n",
    "test_on_batch = generate_data(test, batch_size)\n",
    "train_steps_per_epoch = len(train) // batch_size\n",
    "test_steps_per_epoch = len(test) // batch_size\n",
    "train_steps_per_epoch, test_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 61)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 300)     2400000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 61, 300)      2400000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 256), (None, 571392      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        [(None, 61, 256), (N 571392      embedding_2[0][0]                \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 61, 8000)     2056000     cu_dnnlstm_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,998,784\n",
      "Trainable params: 7,998,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "hidden_dims = 256\n",
    "emb_dims = 300\n",
    "\n",
    "##training network architecture\n",
    "enc_inputs = Input(shape=(max_enc_seq,))\n",
    "enc_emb = Embedding(vocab_size, emb_dims)(enc_inputs)\n",
    "enc = CuDNNLSTM(hidden_dims, return_state=True)\n",
    "_, state_h, state_c = enc(enc_emb)\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "dec_inputs = Input(shape=(max_dec_seq,))\n",
    "dec_emb = Embedding(vocab_size, emb_dims)(dec_inputs)\n",
    "#return_state is used when the model inferences\n",
    "dec = CuDNNLSTM(hidden_dims, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec(dec_emb, initial_state=enc_states)\n",
    "dec_dense = Dense(8000, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"seq2seq.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 48s 85ms/step - loss: 1.2097 - acc: 0.8351 - val_loss: 0.9368 - val_acc: 0.8503\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.8785 - acc: 0.8565 - val_loss: 0.8429 - val_acc: 0.8609\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.7945 - acc: 0.8665 - val_loss: 0.7770 - val_acc: 0.8687\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.7361 - acc: 0.8729 - val_loss: 0.7356 - val_acc: 0.8735\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.6931 - acc: 0.8774 - val_loss: 0.7065 - val_acc: 0.8770\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 41s 74ms/step - loss: 0.6604 - acc: 0.8808 - val_loss: 0.7690 - val_acc: 0.8734\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff21613b00>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=1, verbose=1, mode=\"auto\")\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_on_batch,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_on_batch,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_1/strided_slice_16:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'cu_dnnlstm_1/strided_slice_17:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nmt_seq2seq.h5\", include_optimizer=False)\n",
    "print(\"The model is saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"nmt_seq2seq.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 128, 300)          2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     [(None, 256), (None, 256) 571392    \n",
      "=================================================================\n",
      "Total params: 2,971,392\n",
      "Trainable params: 2,971,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##inference network architecture\n",
    "enc_model = Model(enc_inputs, enc_states)\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"enc_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 61)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 61, 300)      2400000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        [(None, 61, 256), (N 571392      embedding_2[0][0]                \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 61, 8000)     2056000     cu_dnnlstm_2[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,027,392\n",
      "Trainable params: 5,027,392\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec_input_state_h = Input(shape=(hidden_dims,))\n",
    "dec_input_state_c = Input(shape=(hidden_dims,))\n",
    "dec_input_states = [dec_input_state_h, dec_input_state_c]\n",
    "dec_outputs, state_h, state_c = dec(dec_emb, initial_state=dec_input_states)\n",
    "dec_states = [state_h, state_c]\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "dec_model = Model([dec_inputs] + dec_input_states,\n",
    "                 [dec_outputs] + dec_states)\n",
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"dec_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻訳したい英語を入力してください。:(応答文)翻訳の精度をテストしたいです。\n"
     ]
    }
   ],
   "source": [
    "input_text = input(\"翻訳したい英語を入力してください。:(応答文)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4205, 4, 1752, 338, 8, 3031, 17, 41, 14, 156, 5]\n"
     ]
    }
   ],
   "source": [
    "test = tokenizer.EncodeAsIds(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_text):\n",
    "    tokenized_ids = tokenizer.EncodeAsIds(input_text)\n",
    "    enc_input = pad_or_truncate_inputs(tokenized_ids, max_enc_seq)\n",
    "    \n",
    "    \n",
    "    enc_states = enc_model.predict(enc_input)\n",
    "    output_seq = [start_token_id]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
