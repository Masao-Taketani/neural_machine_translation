{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, CuDNNLSTM, Embedding, Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 100\n",
    "hidden_dims = 256\n",
    "\n",
    "data_file = \"jpn.txt\"\n",
    "enc_input_tokens = []\n",
    "dec_input_tokens = []\n",
    "dec_target_tokens = []\n",
    "start_token_id = 1\n",
    "end_token_id = 2\n",
    "\n",
    "with open(data_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines_list = f.read().split(\"\\n\")\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"spm.model\")\n",
    "\n",
    "for line in lines_list:\n",
    "    #for the last black data, we need to skip\n",
    "    if line == \"\":\n",
    "        break\n",
    "    source_text, target_text = line.split(\"\\t\")\n",
    "    tokenized_source_text = tokenizer.EncodeAsPieces(source_text)\n",
    "    #test = tokenizer.EncodeAsIds(source_text)\n",
    "    #print(test)\n",
    "    tokenized_target_text = tokenizer.EncodeAsPieces(target_text)\n",
    "    \n",
    "    int_tokenized_source = []\n",
    "    int_tokenized_input_target = []\n",
    "    int_tokenized_output_target = []\n",
    "    for token in tokenized_source_text:\n",
    "        int_tokenized_source.append(tokenizer.piece_to_id(token))\n",
    "    for i, token in enumerate(tokenized_target_text):\n",
    "        if i == 0:\n",
    "            int_tokenized_input_target.append(start_token_id)\n",
    "            continue\n",
    "        int_tokenized_input_target.append(tokenizer.piece_to_id(token))\n",
    "        int_tokenized_output_target.append(tokenizer.piece_to_id(token))\n",
    "        \n",
    "    int_tokenized_output_target.append(end_token_id)\n",
    "    \n",
    "    if len(int_tokenized_output_target) != len(int_tokenized_input_target):\n",
    "        print(\"Error\")\n",
    "        \n",
    "    enc_input_tokens.append(int_tokenized_source)\n",
    "    dec_input_tokens.append(int_tokenized_input_target)\n",
    "    dec_target_tokens.append(int_tokenized_output_target)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_enc_seq = max([len(i) for i in enc_input_tokens])\n",
    "max_dec_seq = max([len(i) for i in dec_input_tokens])\n",
    "\n",
    "max_enc_seq, max_dec_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "    pad_id = 0\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(pad_id)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45093/45093 [00:00<00:00, 46074.02it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 84133.82it/s]\n",
      "100%|██████████| 45093/45093 [00:00<00:00, 87501.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(45093, 45093, 45093)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input_tokens = pad_or_truncate_inputs(enc_input_tokens, max_enc_seq)\n",
    "dec_input_tokens = pad_or_truncate_inputs(dec_input_tokens, max_dec_seq)\n",
    "dec_target_tokens = pad_or_truncate_inputs(dec_target_tokens, max_dec_seq)\n",
    "\n",
    "len(enc_input_tokens), len(dec_input_tokens), len(dec_target_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "def shuffle_dataset_and_split_into_train_test(enc_input, dec_input, dec_target, test_ratio=0.2):\n",
    "    dataset_list = list(zip(enc_input, dec_input, dec_target))\n",
    "    np.random.shuffle(dataset_list)\n",
    "    split_point = int(len(enc_input) * test_ratio)\n",
    "    test = dataset_list[:split_point]\n",
    "    train = dataset_list[split_point:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36075, 9018, 3, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = shuffle_dataset_and_split_into_train_test(enc_input_tokens, dec_input_tokens, dec_target_tokens)\n",
    "len(train), len(test), len(train[0]), len(test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 61, 61)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0][0]), len(train[0][1]), len(train[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(data_list, batch_size, shuffle=False):\n",
    "    while True:\n",
    "        if shuffle:\n",
    "            np.random.shuffle(data_list)\n",
    "            \n",
    "        for i in range(0, len(data_list), batch_size):\n",
    "            enc_input_list = []\n",
    "            dec_input_list = []\n",
    "            dec_target_list = []\n",
    "            batch_list_inside_tuples = data_list[i: i + batch_size]\n",
    "            \n",
    "            for sample in batch_list_inside_tuples:\n",
    "                e_inp, d_inp, d_tar = sample[0], sample[1], sample[2]\n",
    "                enc_input_list.append(e_inp)\n",
    "                dec_input_list.append(d_inp)\n",
    "                dec_target_list.append(d_tar)\n",
    "            np_batch_enc_input = np.vstack(enc_input_list)\n",
    "            np_batch_dec_input = np.vstack(dec_input_list)\n",
    "            np_batch_dec_target = np.vstack(dec_target_list)\n",
    "            np_batch_dec_target_one_hot = to_categorical(np_batch_dec_target, num_classes=vocab_size)\n",
    "            ##input values are inside of [], and the rest is output value\n",
    "            yield [np_batch_enc_input, np_batch_dec_input], np_batch_dec_target_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(563, 140)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_on_batch = generate_data(train, batch_size, shuffle=True)\n",
    "test_on_batch = generate_data(test, batch_size)\n",
    "train_steps_per_epoch = len(train) // batch_size\n",
    "test_steps_per_epoch = len(test) // batch_size\n",
    "train_steps_per_epoch, test_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 61)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 128, 300)     2400000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 61, 300)      2400000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 256), (None, 571392      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        [(None, 61, 256), (N 571392      embedding_2[0][0]                \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 61, 8000)     2056000     cu_dnnlstm_2[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 7,998,784\n",
      "Trainable params: 7,998,784\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "hidden_dims = 256\n",
    "emb_dims = 300\n",
    "\n",
    "##training network architecture\n",
    "enc_inputs = Input(shape=(max_enc_seq,))\n",
    "enc_emb = Embedding(vocab_size, emb_dims)(enc_inputs)\n",
    "enc = CuDNNLSTM(hidden_dims, return_state=True)\n",
    "_, state_h, state_c = enc(enc_emb)\n",
    "enc_states = [state_h, state_c]\n",
    "\n",
    "dec_inputs = Input(shape=(max_dec_seq,))\n",
    "dec_emb = Embedding(vocab_size, emb_dims)(dec_inputs)\n",
    "#return_state is used when the model inferences\n",
    "dec = CuDNNLSTM(hidden_dims, return_sequences=True, return_state=True)\n",
    "dec_outputs, _, _ = dec(dec_emb, initial_state=enc_states)\n",
    "dec_dense = Dense(8000, activation=\"softmax\")\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "\n",
    "model = Model([enc_inputs, dec_inputs], dec_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"seq2seq.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "563/563 [==============================] - 55s 97ms/step - loss: 1.2131 - acc: 0.8350 - val_loss: 0.9345 - val_acc: 0.8507\n",
      "Epoch 2/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.8760 - acc: 0.8567 - val_loss: 0.8409 - val_acc: 0.8609\n",
      "Epoch 3/100\n",
      "563/563 [==============================] - 44s 77ms/step - loss: 0.7929 - acc: 0.8665 - val_loss: 0.7758 - val_acc: 0.8688\n",
      "Epoch 4/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.7351 - acc: 0.8729 - val_loss: 0.7331 - val_acc: 0.8742 0.7356 -\n",
      "Epoch 5/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.6917 - acc: 0.8776 - val_loss: 0.7022 - val_acc: 0.8780\n",
      "Epoch 6/100\n",
      "563/563 [==============================] - 42s 75ms/step - loss: 0.6531 - acc: 0.8827 - val_loss: 0.6702 - val_acc: 0.8836\n",
      "Epoch 7/100\n",
      "563/563 [==============================] - 44s 77ms/step - loss: 0.6172 - acc: 0.8878 - val_loss: 0.6505 - val_acc: 0.8860\n",
      "Epoch 8/100\n",
      "563/563 [==============================] - 44s 78ms/step - loss: 0.5908 - acc: 0.8913 - val_loss: 0.6328 - val_acc: 0.8888\n",
      "Epoch 9/100\n",
      "563/563 [==============================] - 44s 78ms/step - loss: 0.5668 - acc: 0.8941 - val_loss: 0.6215 - val_acc: 0.8904\n",
      "Epoch 10/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.5471 - acc: 0.8966 - val_loss: 0.6128 - val_acc: 0.8916\n",
      "Epoch 11/100\n",
      "563/563 [==============================] - 43s 76ms/step - loss: 0.5289 - acc: 0.8988 - val_loss: 0.6054 - val_acc: 0.8930\n",
      "Epoch 12/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.5123 - acc: 0.9010 - val_loss: 0.5982 - val_acc: 0.8942\n",
      "Epoch 13/100\n",
      "563/563 [==============================] - 44s 77ms/step - loss: 0.4967 - acc: 0.9030 - val_loss: 0.5936 - val_acc: 0.8949\n",
      "Epoch 14/100\n",
      "563/563 [==============================] - 43s 76ms/step - loss: 0.4828 - acc: 0.9049 - val_loss: 0.5908 - val_acc: 0.8955\n",
      "Epoch 15/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.4691 - acc: 0.9069 - val_loss: 0.5866 - val_acc: 0.8965\n",
      "Epoch 16/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.4573 - acc: 0.9087 - val_loss: 0.5865 - val_acc: 0.8967\n",
      "Epoch 17/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.4489 - acc: 0.9098 - val_loss: 0.5827 - val_acc: 0.8973\n",
      "Epoch 18/100\n",
      "563/563 [==============================] - 43s 77ms/step - loss: 0.4343 - acc: 0.9122 - val_loss: 0.5810 - val_acc: 0.8982\n",
      "Epoch 19/100\n",
      "563/563 [==============================] - 44s 79ms/step - loss: 0.4235 - acc: 0.9141 - val_loss: 0.5792 - val_acc: 0.8988\n",
      "Epoch 20/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.4134 - acc: 0.9158 - val_loss: 0.5782 - val_acc: 0.8988\n",
      "Epoch 21/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.4030 - acc: 0.9175 - val_loss: 0.5780 - val_acc: 0.8997\n",
      "Epoch 22/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.3944 - acc: 0.9190 - val_loss: 0.5780 - val_acc: 0.8997\n",
      "Epoch 23/100\n",
      "563/563 [==============================] - 42s 74ms/step - loss: 0.3849 - acc: 0.9207 - val_loss: 0.5772 - val_acc: 0.9001\n",
      "Epoch 24/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.3762 - acc: 0.9223 - val_loss: 0.5779 - val_acc: 0.9005\n",
      "Epoch 25/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.3675 - acc: 0.9239 - val_loss: 0.5769 - val_acc: 0.9010\n",
      "Epoch 26/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.3587 - acc: 0.9256 - val_loss: 0.5785 - val_acc: 0.9011\n",
      "Epoch 27/100\n",
      "563/563 [==============================] - 41s 73ms/step - loss: 0.3520 - acc: 0.9268 - val_loss: 0.5796 - val_acc: 0.9015\n",
      "Epoch 00027: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fff4bcad390>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping = EarlyStopping(monitor=\"val_loss\", patience=1, verbose=1, mode=\"auto\")\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=train_on_batch,\n",
    "    steps_per_epoch=train_steps_per_epoch,\n",
    "    epochs=epochs,\n",
    "    verbose=1,\n",
    "    callbacks=[earlystopping],\n",
    "    validation_data=test_on_batch,\n",
    "    validation_steps=test_steps_per_epoch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/network.py:877: UserWarning: Layer cu_dnnlstm_2 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'cu_dnnlstm_1/strided_slice_16:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'cu_dnnlstm_1/strided_slice_17:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save(\"nmt_seq2seq.h5\", include_optimizer=False)\n",
    "print(\"The model is saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"nmt_seq2seq.h5\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 128, 300)          2400000   \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)     [(None, 256), (None, 256) 571392    \n",
      "=================================================================\n",
      "Total params: 2,971,392\n",
      "Trainable params: 2,971,392\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "##inference network architecture\n",
    "enc_model = Model(enc_inputs, enc_states)\n",
    "enc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(enc_model, to_file=\"enc_model_for_inference.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer cu_dnnlstm_2 expects 7 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'embedding_2/embedding_lookup:0' shape=(?, 61, 300) dtype=float32>, <tf.Tensor 'input_7:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_8:0' shape=(?, 256) dtype=float32>]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-09690fd1c93c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdec_input_state_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdec_input_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdec_input_state_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_input_state_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdec_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdec_input_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdec_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstate_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_c\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdec_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0moriginal_input_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_input_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;31m# Raise exceptions in case the input is not compatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m             \u001b[0;31m# with the input_spec set at build time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_input_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[0;31m# Handle mask propagation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    297\u001b[0m                              \u001b[0;34m'but it received '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                              \u001b[0;34m' input tensors. Input received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                              str(inputs))\n\u001b[0m\u001b[1;32m    300\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minput_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Layer cu_dnnlstm_2 expects 7 inputs, but it received 3 input tensors. Input received: [<tf.Tensor 'embedding_2/embedding_lookup:0' shape=(?, 61, 300) dtype=float32>, <tf.Tensor 'input_7:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'input_8:0' shape=(?, 256) dtype=float32>]"
     ]
    }
   ],
   "source": [
    "dec_input_state_h = Input(shape=(hidden_dims,))\n",
    "dec_input_state_c = Input(shape=(hidden_dims,))\n",
    "dec_input_states = [dec_input_state_h, dec_input_state_c]\n",
    "dec_outputs, state_h, state_c = dec(dec_emb, initial_state=dec_input_states)\n",
    "dec_states = [state_h, state_c]\n",
    "dec_outputs = dec_dense(dec_outputs)\n",
    "dec_model = Model([dec_inputs] + dec_input_states,\n",
    "                 [dec_outputs] + dec_states)\n",
    "dec_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "翻訳したい英語を入力してください。:(応答文)翻訳の精度をテストしたいです。\n"
     ]
    }
   ],
   "source": [
    "input_text = input(\"翻訳したい英語を入力してください。:(応答文)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 4205, 4, 1752, 338, 8, 3031, 17, 41, 14, 156, 5]\n"
     ]
    }
   ],
   "source": [
    "test = tokenizer.EncodeAsIds(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(input_text):\n",
    "    tokenized_ids = tokenizer.EncodeAsIds(input_text)\n",
    "    enc_input = pad_or_truncate_inputs(tokenized_ids, max_enc_seq)\n",
    "    \n",
    "    \n",
    "    enc_states = enc_model.predict(enc_input)\n",
    "    output_seq = [start_token_id]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
